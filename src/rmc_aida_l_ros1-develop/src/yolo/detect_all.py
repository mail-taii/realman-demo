#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆYOLOæ£€æµ‹å’Œåˆ†å‰²ç¨‹åº
æ£€æµ‹å›¾åƒï¼šæ˜¾ç¤ºæ‰€æœ‰è¯†åˆ«åˆ°çš„ç‰©ä½“
åˆ†å‰²å›¾åƒï¼šåªåˆ†å‰²ç½®ä¿¡åº¦æœ€é«˜çš„ç‰©ä½“
"""

import cv2
import numpy as np
import torch
from ultralytics import YOLO
from ultralytics.models.sam import Predictor as SAMPredictor

# åŠ è½½yoloæ¨¡å‹
model = YOLO("/home/rm/yolo/best1.pt")
# åŠ è½½SAMæ¨¡å‹
model_weight = '/home/rm/yolo/sam_b.pt'
predictor = SAMPredictor(overrides=dict(conf=0.25, model=model_weight, save=False))

def detect_all_objects(image, conf_threshold=0.1):
    """
    æ£€æµ‹å›¾åƒä¸­çš„æ‰€æœ‰ç‰©ä½“ï¼Œè¿”å›æ‰€æœ‰æœ‰æ•ˆæ£€æµ‹æ¡†å’Œç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹æ¡†
    """
    results = model.predict(image, device='cuda' if torch.cuda.is_available() else 'cpu')
    
    # è·å–æ£€æµ‹ç»“æœ
    boxes = results[0].boxes
    labels = results[0].names
    
    # è·å–æ‰€æœ‰æœ‰æ•ˆçš„æ£€æµ‹æ¡†
    all_valid_boxes = []
    all_detected_classes = []
    
    if boxes is not None:
        for idx, box in enumerate(boxes):
            if box.conf > conf_threshold:
                class_id = int(box.cls[0].item())
                class_name = labels[class_id]
                confidence = box.conf.item()
                
                all_valid_boxes.append({
                    'box': box,
                    'class_name': class_name,
                    'confidence': confidence,
                    'class_id': class_id
                })
                all_detected_classes.append(class_name)
    
    print(f"æ£€æµ‹åˆ°æ‰€æœ‰ç±»åˆ«: {all_detected_classes}")
    print(f"æ€»å…±æ£€æµ‹åˆ° {len(all_valid_boxes)} ä¸ªç‰©ä½“")
    
    # æ‰¾åˆ°ç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹æ¡†
    max_conf_detection = None
    if all_valid_boxes:
        max_conf_detection = max(all_valid_boxes, key=lambda x: x['confidence'])
        print(f"ç½®ä¿¡åº¦æœ€é«˜çš„ç‰©ä½“: {max_conf_detection['class_name']} (ç½®ä¿¡åº¦: {max_conf_detection['confidence']:.2f})")
    
    return all_valid_boxes, max_conf_detection

def draw_all_detections(image, all_detections):
    """
    åœ¨å›¾åƒä¸Šç»˜åˆ¶æ‰€æœ‰æ£€æµ‹åˆ°çš„ç‰©ä½“
    """
    annotated_image = image.copy()
    
    # å®šä¹‰é¢œè‰²åˆ—è¡¨ï¼Œä¸ºä¸åŒç±»åˆ«åˆ†é…ä¸åŒé¢œè‰²
    colors = [
        (0, 255, 0),    # ç»¿è‰²
        (255, 0, 0),    # è“è‰²
        (0, 0, 255),    # çº¢è‰²
        (255, 255, 0),  # é’è‰²
        (255, 0, 255),  # å“çº¢è‰²
        (0, 255, 255),  # é»„è‰²
        (128, 0, 128),  # ç´«è‰²
        (255, 165, 0),  # æ©™è‰²
    ]
    
    for i, detection in enumerate(all_detections):
        box = detection['box']
        class_name = detection['class_name']
        confidence = detection['confidence']
        
        # è·å–è¾¹ç•Œæ¡†åæ ‡
        x1, y1, x2, y2 = box.xyxy[0]
        
        # é€‰æ‹©é¢œè‰²
        color = colors[i % len(colors)]
        
        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        cv2.rectangle(annotated_image, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)
        
        # ç»˜åˆ¶æ ‡ç­¾
        label = f"{class_name}: {confidence:.2f}"
        
        # è®¡ç®—æ–‡æœ¬å¤§å°ä»¥ç»˜åˆ¶èƒŒæ™¯
        (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)
        
        # ç»˜åˆ¶æ–‡æœ¬èƒŒæ™¯
        cv2.rectangle(annotated_image, 
                     (int(x1), int(y1) - text_height - baseline - 5),
                     (int(x1) + text_width, int(y1)),
                     color, -1)
        
        # ç»˜åˆ¶æ–‡æœ¬
        cv2.putText(annotated_image, label,
                   (int(x1), int(y1) - baseline - 5),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        print(f"  {i+1}. {class_name}: {confidence:.2f}")
    
    return annotated_image

def process_sam_results(results):
    """
    å¤„ç†SAMåˆ†å‰²ç»“æœï¼Œè¿”å›ä¸­å¿ƒç‚¹å’Œæ©ç 
    """
    center = (0, 0)
    mask = None

    if results[0].masks is not None:
        for index, contour in enumerate(results[0].masks.xy):
            contour = contour.astype(np.int32)
            rect = cv2.minAreaRect(contour)
            center, wh, angle = rect
            center = list(map(int, center))

        masks = results[0].masks.data.clone()
        if isinstance(masks[0], torch.Tensor):
            masks = np.array(masks.cpu())
        
        mask = np.zeros_like(masks[0], dtype=np.uint8)
        for i in range(len(masks)):
            mask = cv2.morphologyEx(masks[i].astype(np.uint8), cv2.MORPH_CLOSE, np.ones((3, 3), np.uint8))
            mask[mask == 1] = 255

    return center, mask

def segment_image(color_img, output_path='segmentation_result.png', 
                          draw_path='detection_result.png'):
    """
    å¢å¼ºç‰ˆå›¾åƒåˆ†å‰²å‡½æ•°
    æ£€æµ‹å›¾åƒï¼šæ˜¾ç¤ºæ‰€æœ‰ç‰©ä½“
    åˆ†å‰²å›¾åƒï¼šåªåˆ†å‰²ç½®ä¿¡åº¦æœ€é«˜çš„ç‰©ä½“
    """
    
    # æ£€æµ‹æ‰€æœ‰ç‰©ä½“
    all_detections, max_conf_detection = detect_all_objects(color_img)
    
    if not all_detections:
        print("æœªæ£€æµ‹åˆ°ä»»ä½•æœ‰æ•ˆç‰©ä½“")
        return color_img
    
    # ç»˜åˆ¶æ‰€æœ‰æ£€æµ‹ç»“æœ
    detection_image = draw_all_detections(color_img, all_detections)
    cv2.imwrite(draw_path, detection_image)
    print(f"âœ… æ£€æµ‹ç»“æœå·²ä¿å­˜: {draw_path}")
    
    # åªå¯¹ç½®ä¿¡åº¦æœ€é«˜çš„ç‰©ä½“è¿›è¡Œåˆ†å‰²
    if max_conf_detection:
        try:
            # è·å–ç½®ä¿¡åº¦æœ€é«˜ç‰©ä½“çš„ä¸­å¿ƒç‚¹
            box = max_conf_detection['box']
            x1, y1, x2, y2 = box.xyxy[0]
            x_center = (x1 + x2) / 2
            y_center = (y1 + y2) / 2
            center = (x_center, y_center)
            
            print(f"ğŸ”„ å¯¹ç½®ä¿¡åº¦æœ€é«˜çš„ç‰©ä½“è¿›è¡Œåˆ†å‰²: {max_conf_detection['class_name']}")
            
            # ä½¿ç”¨SAMè¿›è¡Œåˆ†å‰²
            results = predictor(color_img, points=center, labels=[1])
            center, mask = process_sam_results(results)
            
            if mask is not None:
                cv2.imwrite(output_path, mask)
                print(f"âœ… åˆ†å‰²ç»“æœå·²ä¿å­˜: {output_path}")
            else:
                print("âŒ æœªèƒ½ç”Ÿæˆåˆ†å‰²æ©ç ")
                
        except Exception as e:
            print(f"âŒ åˆ†å‰²è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    return mask

def main():
    """
    ä¸»å‡½æ•°ï¼šå¤„ç†å•å¼ å›¾ç‰‡
    """
    source = "/home/rm/yolo/origin.png"
    
    if not os.path.exists(source):
        print(f"âŒ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {source}")
        return
    
    print(f"ğŸ”„ å¤„ç†å›¾ç‰‡: {source}")
    image = cv2.imread(source)
    
    if image is None:
        print(f"âŒ æ— æ³•è¯»å–å›¾ç‰‡: {source}")
        return
    
    # è°ƒç”¨å¢å¼ºç‰ˆåˆ†å‰²å‡½æ•°
    result_image = segment_image(image)
    print("ğŸ‰ å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    import os
    main()
