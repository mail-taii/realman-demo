#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
"""
æ‘„åƒå¤´æ•°æ®è®¢é˜…å®¢æˆ·ç«¯ - ä¸ºtask_planningæä¾›æ‘„åƒå¤´å›¾åƒæ¥å£

åŠŸèƒ½ï¼š
1. è®¢é˜…D435æ‘„åƒå¤´çš„colorå’Œdepthå›¾åƒtopic
2. å®æ—¶æ˜¾ç¤ºæŒ‡å®šæ‘„åƒå¤´çš„colorç”»é¢
3. ä¿å­˜æŒ‡å®šæ‘„åƒå¤´çš„å½“å‰å¸§RGBå’Œæ·±åº¦å›¾åƒåˆ°imagesæ–‡ä»¶å¤¹ï¼ˆè¦†ç›–ä¸Šä¸€å¸§ï¼‰
"""

import rospy
from sensor_msgs.msg import Image
import cv2
from cv_bridge import CvBridge
import os
import threading
import numpy as np
import torch
from ultralytics import YOLO
from ultralytics.models.sam import Predictor as SAMPredictor


class CameraClient:
    def __init__(self):
        self.bridge = CvBridge()
        self.image_data = {
            '0': {'color': None, 'depth': None},
            '1': {'color': None, 'depth': None},
            '2': {'color': None, 'depth': None}
        }
        self.subs = []
        self.lock = threading.Lock()
        # è®¢é˜…æ‰€æœ‰æ‘„åƒå¤´çš„colorå’Œdepth
        for i in range(3):
            color_topic = f"/camera_d435_{i}/color/image_raw"
            depth_topic = f"/camera_d435_{i}/depth/image_raw"
            self.subs.append(rospy.Subscriber(color_topic, Image, self._make_callback(i, 'color')))
            self.subs.append(rospy.Subscriber(depth_topic, Image, self._make_callback(i, 'depth')))

        # åŠ è½½yoloæ¨¡å‹
        self.model = YOLO(os.path.join(os.path.dirname(os.path.abspath(__file__)), "../../yolo/best1.pt"))
        # åŠ è½½SAMæ¨¡å‹
        self.predictor = SAMPredictor(overrides=dict(conf=0.25, 
                                                     model=os.path.join(os.path.dirname(os.path.abspath(__file__)),'../../yolo/sam_b.pt'),
                                                     save=False
                                                     )
        )

        # è®¾ç½®imagesæ–‡ä»¶å¤¹ä¸ºå½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•ä¸‹
        self.image_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../images')

    def _make_callback(self, idx, typ):
        def callback(msg):
            cv_img = self.bridge.imgmsg_to_cv2(msg, "bgr8" if typ == 'color' else "passthrough")
            with self.lock:
                self.image_data[str(idx)][typ] = cv_img
        return callback

    def show_camera(self, camera_ids):
        """
        å®æ—¶æ˜¾ç¤ºæŒ‡å®šæ‘„åƒå¤´çš„colorç”»é¢
        camera_ids: list, å¦‚ ['0'], ['1','2']
        """
        while not rospy.is_shutdown():
            with self.lock:
                for cid in camera_ids:
                    img = self.image_data[cid]['color']
                    if img is not None:
                        cv2.imshow(f"Camera {cid} Color", img)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        cv2.destroyAllWindows()

    def save_images(self, camera_ids):
        """
        ä¿å­˜æŒ‡å®šæ‘„åƒå¤´çš„å½“å‰å¸§RGBå’Œæ·±åº¦å›¾åƒåˆ°imagesæ–‡ä»¶å¤¹
        camera_ids: list, å¦‚ ['0'], ['1','2']
        """

        with self.lock:
            for cid in camera_ids:
                color_img = self.image_data[cid]['color']
                depth_img = self.image_data[cid]['depth']
                if color_img is not None:
                    cv2.imwrite(os.path.join(self.image_path, f"camera_{cid}_color.png"), color_img)
                if depth_img is not None:
                    cv2.imwrite(os.path.join(self.image_path, f"camera_{cid}_depth.png"), depth_img)

    def detect_all_objects(self, image, conf_threshold=0.1):
        """
        æ£€æµ‹å›¾åƒä¸­çš„æ‰€æœ‰ç‰©ä½“ï¼Œè¿”å›æ‰€æœ‰æœ‰æ•ˆæ£€æµ‹æ¡†å’Œç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹æ¡†
        """
        results = self.model.predict(image, device='cuda' if torch.cuda.is_available() else 'cpu')
        
        # è·å–æ£€æµ‹ç»“æœ
        boxes = results[0].boxes
        labels = results[0].names
        
        # è·å–æ‰€æœ‰æœ‰æ•ˆçš„æ£€æµ‹æ¡†
        all_valid_boxes = []
        all_detected_classes = []
        
        if boxes is not None:
            for idx, box in enumerate(boxes):
                if box.conf > conf_threshold:
                    class_id = int(box.cls[0].item())
                    class_name = labels[class_id]
                    confidence = box.conf.item()
                    
                    all_valid_boxes.append({
                        'box': box,
                        'class_name': class_name,
                        'confidence': confidence,
                        'class_id': class_id
                    })
                    all_detected_classes.append(class_name)
        
        print(f"æ£€æµ‹åˆ°æ‰€æœ‰ç±»åˆ«: {all_detected_classes}")
        print(f"æ€»å…±æ£€æµ‹åˆ° {len(all_valid_boxes)} ä¸ªç‰©ä½“")
        
        # æ‰¾åˆ°ç½®ä¿¡åº¦æœ€é«˜çš„æ£€æµ‹æ¡†
        max_conf_detection = None
        if all_valid_boxes:
            max_conf_detection = max(all_valid_boxes, key=lambda x: x['confidence'])
            print(f"ç½®ä¿¡åº¦æœ€é«˜çš„ç‰©ä½“: {max_conf_detection['class_name']} (ç½®ä¿¡åº¦: {max_conf_detection['confidence']:.2f})")
        
        return all_valid_boxes, max_conf_detection

    def draw_all_detections(self, image, all_detections):
        """
        åœ¨å›¾åƒä¸Šç»˜åˆ¶æ‰€æœ‰æ£€æµ‹åˆ°çš„ç‰©ä½“
        """
        annotated_image = image.copy()
        
        # å®šä¹‰é¢œè‰²åˆ—è¡¨ï¼Œä¸ºä¸åŒç±»åˆ«åˆ†é…ä¸åŒé¢œè‰²
        colors = [
            (0, 255, 0),    # ç»¿è‰²
            (255, 0, 0),    # è“è‰²
            (0, 0, 255),    # çº¢è‰²
            (255, 255, 0),  # é’è‰²
            (255, 0, 255),  # å“çº¢è‰²
            (0, 255, 255),  # é»„è‰²
            (128, 0, 128),  # ç´«è‰²
            (255, 165, 0),  # æ©™è‰²
        ]
        
        for i, detection in enumerate(all_detections):
            box = detection['box']
            class_name = detection['class_name']
            confidence = detection['confidence']
            
            # è·å–è¾¹ç•Œæ¡†åæ ‡
            x1, y1, x2, y2 = box.xyxy[0]
            
            # é€‰æ‹©é¢œè‰²
            color = colors[i % len(colors)]
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(annotated_image, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)
            
            # ç»˜åˆ¶æ ‡ç­¾
            label = f"{class_name}: {confidence:.2f}"
            
            # è®¡ç®—æ–‡æœ¬å¤§å°ä»¥ç»˜åˆ¶èƒŒæ™¯
            (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)
            
            # ç»˜åˆ¶æ–‡æœ¬èƒŒæ™¯
            cv2.rectangle(annotated_image, 
                        (int(x1), int(y1) - text_height - baseline - 5),
                        (int(x1) + text_width, int(y1)),
                        color, -1)
            
            # ç»˜åˆ¶æ–‡æœ¬
            cv2.putText(annotated_image, label,
                    (int(x1), int(y1) - baseline - 5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            print(f"  {i+1}. {class_name}: {confidence:.2f}")
        
        return annotated_image

    def process_sam_results(self, results):
        """
        å¤„ç†SAMåˆ†å‰²ç»“æœï¼Œè¿”å›ä¸­å¿ƒç‚¹å’Œæ©ç 
        """
        center = (0, 0)
        mask = None

        if results[0].masks is not None:
            for index, contour in enumerate(results[0].masks.xy):
                contour = contour.astype(np.int32)
                rect = cv2.minAreaRect(contour)
                center, wh, angle = rect
                center = list(map(int, center))

            masks = results[0].masks.data.clone()
            if isinstance(masks[0], torch.Tensor):
                masks = np.array(masks.cpu())
            
            mask = np.zeros_like(masks[0], dtype=np.uint8)
            for i in range(len(masks)):
                mask = cv2.morphologyEx(masks[i].astype(np.uint8), cv2.MORPH_CLOSE, np.ones((3, 3), np.uint8))
                mask[mask == 1] = 255

        return center, mask

    def segment_image(self, color_img, output_path='segmentation_result.png', 
                            draw_path='detection_result.png'):
        """
        å¢å¼ºç‰ˆå›¾åƒåˆ†å‰²å‡½æ•°
        æ£€æµ‹å›¾åƒï¼šæ˜¾ç¤ºæ‰€æœ‰ç‰©ä½“
        åˆ†å‰²å›¾åƒï¼šåªåˆ†å‰²ç½®ä¿¡åº¦æœ€é«˜çš„ç‰©ä½“
        """        
        # æ£€æµ‹æ‰€æœ‰ç‰©ä½“
        all_detections, max_conf_detection = self.detect_all_objects(color_img)
        
        if not all_detections:
            print("æœªæ£€æµ‹åˆ°ä»»ä½•æœ‰æ•ˆç‰©ä½“")
            return color_img
        
        # ç»˜åˆ¶æ‰€æœ‰æ£€æµ‹ç»“æœ
        detection_image = self.draw_all_detections(color_img, all_detections)
        cv2.imwrite(os.path.join(self.image_path, draw_path), detection_image)
        print(f"âœ… æ£€æµ‹ç»“æœå·²ä¿å­˜: {draw_path}")
        
        # åªå¯¹ç½®ä¿¡åº¦æœ€é«˜çš„ç‰©ä½“è¿›è¡Œåˆ†å‰²
        if max_conf_detection:
            try:
                # è·å–ç½®ä¿¡åº¦æœ€é«˜ç‰©ä½“çš„ä¸­å¿ƒç‚¹
                box = max_conf_detection['box']
                x1, y1, x2, y2 = box.xyxy[0]
                x_center = (x1 + x2) / 2
                y_center = (y1 + y2) / 2
                center = (x_center, y_center)
                
                print(f"ğŸ”„ å¯¹ç½®ä¿¡åº¦æœ€é«˜çš„ç‰©ä½“è¿›è¡Œåˆ†å‰²: {max_conf_detection['class_name']}")
                
                # ä½¿ç”¨SAMè¿›è¡Œåˆ†å‰²
                results = self.predictor(color_img, points=center, labels=[1])
                center, mask = self.process_sam_results(results)
                
                if mask is not None:
                    cv2.imwrite(os.path.join(self.image_path, output_path), mask)
                    print(f"âœ… åˆ†å‰²ç»“æœå·²ä¿å­˜: {output_path}")
                else:
                    print("âŒ æœªèƒ½ç”Ÿæˆåˆ†å‰²æ©ç ")
                    
            except Exception as e:
                print(f"âŒ åˆ†å‰²è¿‡ç¨‹å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
        
        return detection_image

# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    rospy.init_node("camera_client", anonymous=True)
    client = CameraClient()
    # client.show_camera(['0'])
    client.segment_image_realtime(['1'])
